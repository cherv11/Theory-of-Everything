---
layout: page
title: Вероятностно-стратегические схемы как лучший способ организации информации во Вселенной
lang: ru
---

[2]: ../ABIW
[4]: ../MNES
[6]: ../AEGG
[7]: ../BLA
[11]: ../SI 
[15]: ../TMBM 
[Fool trends]: https://tylervigen.com/spurious-correlations 

## Корреляция

Всё, чему можно дать определение, является объектом логики. Следовательно, всё является объектом логики. Но когда мы говорим «яблоко», это может иметь множество разных смыслов:   

1) Яблоко лежит на столе;  
2) Яблоко содержит косточки;  
3) Глазное яблоко поглощает свет.  

В первом случае речь идёт о конкретном яблоке во множестве объектов, находящихся на столе. Во втором мы думаем обо всех яблоках во Вселенной. В последнем же примере глаз — это метафора яблока, вместе с которым они имеют общую *мету*, определение которой — что-то круглое. Это можно представить так: 

<p>
    <img class="pimg" src="../../../images/PSS/1.jpg" width=400>
    <p class="pdesc">Рисунок 1. Три уровня абстракции яблока</p>
</p>

Двойная связь обозначает знак «=» и нужна для визуального удобства. Вы должны возразить, что:

<p>
    <img class="pimg" src="../../../images/PSS/2.jpg" width=400>
    <p class="pdesc">Рисунок 2. Яблоко является частью стола</p>
</p>

Действительно, мысля об утверждении 1, иногда мы считаем яблоко частью стола, отделённого вместе с лежащими на нём предметами от остального пространства. В данном контексте стол — это предметы, лежащие на нём, а также скатерть и грязь на ножке стола. Ровно это и означает связь на первом рисунке: множества конкретного стола и всех яблок пересекаются. Мы называем эту связь *корреляцией*.

**Корреляция — это мера сходства множеств.**

Отныне мы говорим о том, что любая корреляция лежит в промежутке $[0;1]$ вместо общепринятого $[-1;1]$. К примеру, вы захотели найти корреляцию между наличием буллинга и плохими оценками в школах страны. Для этого вы взяли репрезентативную по полу выборку и провели анкетирование в одной конкретной школе, получив чистые ответы на один вопрос: «Подвергались ли вы буллингу в школе?», а также собрали данные о средних оценках. Что вы выяснили?

<p>
    <img class="pimg" src="../../../images/PSS/3.jpg" width=400>
    <p class="pdesc">Рисунок 3. Репрезентативность в исследовании</p>
</p>

Можно посчитать корреляции между буллингом и разными оценками, но нам достаточно одной из них. Также мы не добавили абстрактного ученика, которому принадлежат оценки: именно на его функцию обучения влияет буллинг напрямую. А ещё на диаграмме есть *антонимы* — взаимоисключающие понятия: нам достаточно лишь одного из полов на схеме, так как сумма корреляций $x_i$ и $y_i$ равна $1$. Антонимами являются и оценки, которые мы для простоты разделили на плохие и хорошие, но вообще антонимы — это частный случай *типов* — множеств, представляющих собой разные значения одного параметра (пол, оценка), и поэтому имеющих корреляцию $0$ между собой. Итак, вы измерили корреляцию $w_2$ и можете заявить, что она приблизительно равна $w_1$ по причине репрезентативности: 

<p style="text-align: center">$x_1≈x_2; y_1≈y_2 ⇒ w_1≈w_2$</p>

Но, разумеется, это ошибка. Если вы составите тепловые карты школ в стране по факторам, предположительно связанным с буллингом и уровнем оценок, вы получите следующее:

<p>
    <img class="pimg" src="../../../images/PSS/4.jpg" width=400>
    <p class="pdesc">Рисунок 4. Факторы, влияющие на корреляцию между буллингом и уровнем оценок</p>
</p>

Объект $α_i^{w_i}$ — любое подмножество в множестве всех школ, содержащее информацию о корреляции между буллингом и плохими оценками. Множеством оценок может быть не только ученик (поэтому мы его не добавили), а вся школа, город, один класс, влияние на одну или все оценdesотдельного ученика всех остальных учеников в школе или конкретная пара учеников — всё это разные уровни абстракции. Факторы, влияющие на корреляцию — суть факторы, влияющие на коррелирующие множества: наличие друзей у получающего оценки и уровень давления родителей на детей, склонных задирать других. Выявив такие факторы, вы можете делать прогнозы об уровне буллинга и справедливости оценок в конкретной школе, из чего, как следствие, можно прогнозировать корреляцию $w_i$. Это позволяет также выявить теоремы: в школе с дружелюбными детьми плохих оценок меньше; в школе, где родители больше давят на детей, уровень буллинга выше.

Если у вас нет гипотез, связанных с гендерным различием в данной системе, фактор пола может лишь незначительно влиять на искомую корреляцию. Факторы, дающие репрезентативность, на схеме ничем не отличаются от прочих и призваны уравнять множества для чистоты эксперимента. Однако, помимо того, что репрезентативность не помогает экстраполировать данные, количество разнородных данных имеет решающее значение при работе с гипотезами и поиске ошибок (см. ниже) в теоремах, поэтому выбирая между малой репрезентативной и большой нерепрезентативной выборками, следует отдавать предпочтение второму. Следовательно, значение репрезентативности выборки по базовым факторам переоценено. 

Вот то, чего вы точно не знали о корреляции. Она бывает асимметричной:

<p>
    <img class="pimg" src="../../../images/PSS/5.jpg" width=400>
    <p class="pdesc">Рисунок 5. Асимметричная корреляция</p>
</p>

Дополним наше определение: корреляция $AB$ означает вероятность нахождения произвольного элемента множества $A$ в множестве $B$. Корреляции $r_{AB}$ и $r_{BA}$ равны $\frac{n}{N(A)}$ и $\frac{n}{N(B)}$  соответственно, где $n$ — количество элементов в пересечении $A$ и $B$. Отсюда следует, что корреляция симметрична, когда множества $A$ и $B$ равномощны. 

Действительно, количество учеников, которые подвергаются буллингу, не равно количеству учеников, получивших плохую среднюю оценку. Но что же из этого тогда — искомая корреляция? Зависит от того, что мы хотим найти. Вероятность того, что ученик получит плохую оценку, если на него влияет буллинг — это корреляция (должны ли мы называть это отношением?) в сторону оценки. Так, операция вычисления корреляции имеет направление, которое мы обычно называем направлением *времени*, а цепочка таких операций называется *мысль*. 

## Эффективность

Давайте посчитаем, насколько увеличился шанс получить плохую оценку по сравнению с учениками, которых не буллят. Для создания антонима буллинга воспользуемся логическим отрицанием. Вместе буллинг и антоним составляют множество учеников такое же, какое составляют оценки, потому что это два разных способа поделить одно множество. Поэтому корреляция между ними составляет $1$ в обе стороны. Ещё подпишем количество элементов в множествах и пересечениях:

<p>
    <img class="pimg" src="../../../images/PSS/6.jpg" width=400>
    <p class="pdesc">Рисунок 6. Увеличение вероятности</p>
</p>

То, что сумма корреляций одной связи равна $1$ — совпадение (получается, когда $\frac{n}{N(A)}+\frac{n}{N(B)} = 1 ⇒ n(N(A)+N(B)) = N(A)*N(B)$, что не имеет физического смысла). А вот сумма корреляций к буллингу и небуллингу — результат теоремы: *сумма корреляций к типам от одного множества равна $1$*. Вероятность получить плохую оценку увеличилась на $^2/\_9$, но ни в коем случае это нельзя просто перевести в проценты. Подумайте, насколько увеличится вероятность в $0.9$, если добавить к ней $0.09$, точнее, во сколько раз? Нужно посчитать, во сколько раз она уменьшилась относительно единицы. Из закона убывающей полезности (см. ниже) следует:

<p style="font-size: 24px; text-align: center">
$κ_A^B = \frac{1-r_{\overset{—}{B}A}}{1-r_{BA}} = \frac{^5⁄_9}{^1⁄_3} = \frac{5}{3}$  или $166.(6)%$
</p>

Это называется *эффективностью от включения $B$ в $A$* или же *вкладом $B$ в $A$* (обозначается буквой каппа от англ. contribution). Одинаковые вероятности вносят одинаковый вклад, а физический смысл для значений $≥ 1$ таков: на каждые $κ_A^B$ событий $A$ приходится $κ_A^B-1$ событий, зависящих от $B$, и одно не зависящее. Из каждых $5$ оценок $2$ зависят от буллинга. Вот только наша эффективность немного… отрицательная. Буллинг представляет собой негативное явление, и я не случайно связал его с плохими оценками, а дружбу — с хорошими. Отрицательное явление всегда будет иметь положительную корреляцию с отрицательными явлениями, но законы эффективности те же.

На самом деле, не очень корректно говорить об эффективности по отношению к оценкам. Для этого нужна *функция* (см. [существительные-процессы][7]), отлично подойдёт, например, обучение. У обучения есть эффективность. Но ещё эффективность процесса — это просто вероятность появления объекта. Если на объект влияет несколько вероятностей, то итоговая вероятность определяется по *закону убывающей полезности*, который следует из закономерностей биномиальных коэффициентов:

<p>
    <img class="pimg" src="../../../images/PSS/7.jpg" width=400>
    <p class="pdesc">Рисунок 7. Вероятность объекта</p>
</p>

Однако здесь можно наткнуться на парадокс. Отрицательная корреляция увеличивает итоговую вероятность появления объекта, но не увеличивает эффективность. Решением этого парадокса становится [*теорема страха*][6]: эффективность системы достигает максимума, когда каждый объект имеет только положительные корреляции. То есть каждый объект связан только с объектами той же полярности. Это позволяет избавиться от эмоциональных ошибок (см. [добро и зло, гениальность][6]):

<p>
    <img class="pimg" src="../../../images/PSS/8.jpg" width=400>
    <p class="pdesc">Рисунок 8. Поляризованная система</p>
</p>

Для описания системы целиком, без разложения на составные части, существует *уравнение биполярной эффективности*:

<p>
    <img class="pimg" src="../../../images/PSS/9.jpg" width=400>
    <p class="pdesc">Рисунок 9. Странная система</p>
</p>

Такая система называется *странной* (см. [страх][6]) . Здесь существуют целые цепочки с неизвестной полярной эффективностью. Равные значения эффективности по обоим полярностям из-за отсутствия минуса во второй части дают нам *линию страха* с максимальным значением $0.25$ (горизонтальная), а противоположные — *линию странности* (вертикальная). Максимум функции достигается тогда, когда влияние первого компонента максимизировано, а второго — наоборот:

<p>
    <img class="pimg" src="../../../images/PSS/10.jpg" width=400>
    <p class="pdesc">Рисунок 10. Плоскость биполярной эффективности</p>
</p>

Конечно же, вероятность цепи без узлов равна произведению вероятностей. Как ни странно, сколько бы узлов вы ни имели, вы можете рассчитать любой вклад при условии, что знаете размер полной выборки:

<p>
    <img class="pimg" src="../../../images/PSS/11.jpg" width=400>
    <p class="pdesc">Рисунок 11. Вклад в известной системе</p>
</p>

Есть и обратная сторона: в противном случае для точного вычисления одного значения вклада между двумя множествами надо посчитать долю $B$ во всех множествах, связанных с $A$, для чего нужно буквально знать всё в пределах одной *реальности* (см. [реальность][15]). И что делать? Пользоваться [*теоремой тревоги*][6]: необходимо обращать внимание на все вероятные события вне зависимости от их вклада. Но есть проблема: этой теоремы не хватает для решения задач, связанных с высоким риском. 

## Риск

*Риск $B$ для $A$* — это отношение вклада события $B$ к вероятности этого события, отражающее потенциал $B$ к изменению системы. Например, предположим, что существует рулетка с $9$ красными, $10$ чёрными и одним зелёным полем. Коэффициенты на ставках являются вкладами и составляют $2х$, $1.75х$ и $15х$ соответственно. На что ставим?

<p style="text-align: center">$ρ_A^B = ε_Bκ_A^B$</p>

<p style="text-align: center">$ρ_1=0.45*2=0.9; ρ_2=0.5*1.75=0.875; ρ_3=0.05*15=0.75$</p>

Ни на что. Любой опытный игрок скажет, что $15x$ на зелёное — это очень мало. Потенциал изменения вашего положительного баланса в кармане $A$ отрицательный. Это буквально значит, что, поставив на красное поле $10$ ставок, вы получите обратно $9$. В системе на рисунке изображена функция увеличения баланса на кошельке, а также выигранные в рулетке деньги. Вероятность в $^3/\_7$ означает, что из каждых $7$ рублей $3$ попали к вам в кошелёк (ещё $4$ покрыли расходы на ставку). Потери не коррелируют с функцией увеличения баланса. Применяем формулу вычисления вклада:

<p>
    <img class="pimg" src="../../../images/PSS/12.jpg" width=400>
    <p class="pdesc">Рисунок 12. Коэффициенты в рулетке</p>
</p>

Чтобы выигрывать в такой рулетке, ставя на случайный результат (каждый с вероятностью $^1/\_3$), необходимо, чтобы сумма всех рисков была больше $1$. Если мы будем менять коэффициент только зелёного поля, тогда каждые $30$ ставок дадут выигрыш в $9+8.75+x=30$ при сумме рисков, равной $1$. Да, я случайно изобрёл математическое ожидание, назвав его *коэффициентом удачи системы $A$*:

<p style="text-align: center">$K_A^f = \displaystyle\sum_{i:0 < r_{iA} < 1}{ρ_A^i}$</p>

<p style="text-align: center">$ρ_1=0.45*2=0.9; ρ_2=0.5*1.75=0.875; ρ_3=0.05*24.5=1.225$</p>

Утверждение $r_{iA} > 0$ равносильно $\exists r_{iA}$. Условие $r_{iA} < 1$ нужно для того, чтобы исключить корреляцию $r_{AA}=1$, а также избавиться от всего, что полностью включено в множество $A$, а значит, не зависит от $B$ и приводит к бесконечному значению вклада.

Существует [*теорема надежды*][6]: кажущийся слабым компонент может внести существенный вклад. Она известна вам как эффект бабочки. Объединение её с теоремой тревоги даёт нам [*теорему странности*][6]: необходимо учесть как маловероятные, но значимые события, так и вероятные незначимые. Вы должны беспокоиться о том, что будете есть завтра на ужин, и о том, как защитить Землю от падения астероида, в равной степени. Для этого и нужно понятие риска. Кажется, что в условиях ограниченности информации следует составлять набор значений рисков всех связанных событий и пользоваться правилом трёх сигм.

Для анализа странных систем нужно внести понятие полярного отражения и позитивизации. *Полярное отражение* — это отражение числа от $1$ в континуумах $[0;1]$ и $[1; ∞]$, проще говоря, $\underset{—}{\overset{—}{x}} = \frac{1}{x}$. С этой точки зрения нормализация и вычисление остатка при линейной интерполяции — это сжатие, растяжение или сдвиг ряда чисел до континуума $[0;1]$. А позитивизация — это полярное отражение числа в континуум $[1; ∞]$. Сделаем следующее:

<p style="text-align: center">$\underset{—}{ρ_A^B} = ε_B\underset{—}{κ_A^B}$</p>

Все значения $κ_A^B$ станут больше или равны $1$. Рулетка является поляризованной и, кроме того, полустабильной системой (см. ниже), так что для неё ничего не изменится. Однако теперь значение $\underset{—}{ρ_A^B}$ всегда указывает на важность компонента. Для обычной монетки риски орла и решки $≈ 1$, риск падения на ребро — $ε_B\underset{—}{κ_A^B} = \underset{—}{\overset{—}{\dot{∞}}} * 1 = \underset{—}{\overset{—}{\dot{∞}}}$ (бесконечно мал, или же равен условному нулю), значит, им можно пренебречь. Алгоритм расчёта рисков для любого события может включать в себя рекуррентное вычисление позитивизированного риска $B$ для компонентов $A$, а затем выбор большинства по правилу трёх сигм для следующей итерации.

Для сравнения эффективности двух вариантов системы есть *тождество эффективности*: для любых $A$ и $B$, являющимися вариантами системы $C$, $A$ эффективнее или равно $B$, если для каждого элемента $C$ его вклад в$A$ больше или равен вкладу в $B$:

<p style="text-align: center">$(∀A,B:A,B∈\{x:x=f(C)\})ε_A≥ ε_В?(∀y:∃r_{yC})κ_A^y≥κ_B^y$</p>

## Цепочки

Найти корреляцию между буллингом и оценками. Абсурдная идея: некоторые начинающие исследователи так формулируют цели и гипотезы. Корреляция между множеством всех оценок и буллингом — это вероятность подвергнуться буллингу в школе в целом, так как у всех школьников есть оценки. Эта корреляция равна корреляции между самой школой и буллингом:

<p>
    <img class="pimg" src="../../../images/PSS/13.jpg" width=400>
    <p class="pdesc">Рисунок 13. Ошибка исключения</p>
</p>

Рассмотрим более общий случай. В примере ниже $μ$ — бесконечно малая корреляция, которая появляется, когда мы делим объекты на абстрактные типы по некоторому параметру (см. [существительные-параметры и прилагательные][6]). Количество предметов разных цветов равно условной бесконечности ($\dot{∞}$), а корреляция от него — $\underset{—}{\overset{—}{\dot{∞}}}$. Корреляция вычисляется по правилу треугольника, похожим на векторную сумму. В конкретном примере $γ$ также равно $\underset{—}{\overset{—}{\dot{∞}}}$ (см. [подробнее о знаках $\dot{∞}$ и $\underset{—}{\overset{—}{\dot{∞}}}$][11]). Стоит отметить, что вероятности $β_i$ являются априорными:

<p>
    <img class="pimg" src="../../../images/PSS/14.jpg" width=400>
    <p class="pdesc">Рисунок 14. Треугольник корреляций</p>
</p>

Будем называть связи с корреляциями, равными $1$ в обе стороны, *стабильными*, в одну сторону — *полустабильными*, ни в одну — *нестабильными*. Также введём обозначение связи: $l_{ij}=r_{ij}|r_{ji}$. Умножение работает для треугольника полустабильных связей. Для треугольника нестабильных связей тоже есть общий случай:

<p>
    <img class="pimg" src="../../../images/PSS/15.jpg" width=400>
    <p class="pdesc">Рисунок 15. Теорема треугольника</p>
</p>

Корреляция, близкая к $1$, явно указывает на вероятностную ошибку деления или исключения (см. ниже): объекты в корзине входят в множество яблок, оценки — в множество школы. Однако эти "ошибки" используются для разграничения множеств:

<p>
    <img class="pimg" src="../../../images/PSS/16.jpg" width=400>
    <p class="pdesc">Рисунок 16. Пересечение трёх множеств</p>
</p>

Количество множеств равно известному $2^n-1$, а вот за формулой количества связей стоит гораздо более занимательная комбинаторика. Оно равно сумме количества нестабильных связей, равного количеству комбинаций из всех исходных множеств из $n$ по $2$, и количества полустабильных, равного сумме произведений ряда квазиорбитальных чисел и биноминальных коэффициентов для неполного бинома Ньютона. Корреляций будет ровно вдвое больше. Взгляните на схему для $n=4$:

<p>
    <img class="pimg" src="../../../images/PSS/17.jpg" width=400>
    <p class="pdesc">Рисунок 17. Пересечение четырёх множеств</p>
</p>

Разные цвета — множества и связи разного порядка. Порядок множества равен количеству изначальных множеств, которые пересекает множество. Порядок связи равен большему из порядков множеств, между которыми она проложена. Изначальные множества (синие) имеют порядок $1$, и между ними есть $\frac{n(n-1)}{2}=C_n^2=6$ нестабильных связей порядка $1$. Количество множеств и связей более высоких порядков подчиняется закономерности:

<p>
    <img class="pimg" src="../../../images/PSS/18.jpg" width=400>
    <p class="pdesc">Рисунок 18. Квазиорбитальные числа</p>
</p>

Итого $56$. Физический смысл таков: в $n$ строке треугольника Паскаля первое число — объединение всех множеств — в задаче не рассматривается. Второе число — количество множеств $1$ порядка. Их связи нестабильны, поэтому количество связей равно всем возможным комбинациям исходных множеств по $2$. Остальные числа — количества множеств прочих порядков, каждое из которых формирует $Orb(k)$ полустабильных связей. Для примера рассмотрим множество $4$ порядка (зелёное), которое здесь всего одно. Оно связано со всеми множествами, кроме самого себя, а значит, имеет $2^n-2$ связей. Я назвал такие числа *квазиорбитальными* из-за сходства (но не совпадения) с числом электронов на орбиталях атомов.

Это означает, что существует $56$ квантов — мыслей длины $1$, связывающих $4$ нестабильных множества. Их можно разделить на категории:

1) $A$ связано с $B$ — $A\sim{B}$ (все корреляции $1$ порядка);
2) Часть $A$ является $B$ — $A⊃B$ (повышающая корреляция от множества $n$ порядка к множеству $n+k$ порядка);
3) $A$ является частью $B$ — $A⊂B$ (понижающая корреляция от множества $n+k$ порядка к множеству $n$ порядка).

Число $56$ — количество связей в полном наборе правдивых выводов, которые можно сделать из этих множеств. *Вывод* — это создание новой связи. Вывод $n$ порядка создаёт связи $n$ порядка:

1) $∃x: x∈A, x∈B ⇒ A\sim{B}$ — вывод $1$ порядка;
2) $A\sim{B} ⇒ ∃D: D⊂B, D ⊂А$ — вывод $2$ порядка;
3) $D=A∩B, E=A∩C, F=B∩C ⇒ ∃G: G⊂D, G⊂E, G⊂F$ — вывод $3$ порядка.
Недостающие $3$ связи $3$ порядка проводятся так: $D⊂А, G⊂D ⇒ G⊂A$.  

И так далее. Ряд чисел с количеством возможных выводов из $n$ множеств растёт очень быстро: $\{3,15,56,190,617,1953,6078,18696,57047,173107,523316,1578018,4750293,…\}$, и это только самые простые мысли.

## Ошибки

Система с набором выводов является *полной*, если в ней проложены все правдивые связи. Она также является *правдивой*, если каждая мысль в ней — правда. *Правда* — это мысль, не содержащая ошибок. Ошибки бывают вероятностные, [эмоциональные][6] и [лингвистические][7]. Здесь речь только о первых. Вероятностных ошибки бывают:

<p>
    <img class="pimg" src="../../../images/PSS/19ru.jpg" width=400>
    <p class="pdesc">Рисунок 19. Классификация вероятностных ошибок</p>
</p>

В нашей системе проложены не все возможные связи. Недостающих связей $C_{2^n-1}^2-N(l_{ij})$, для $n=3$ это число $21-15=6$. Это ложные связи:

<p>
    <img class="pimg" src="../../../images/PSS/20.jpg" width=400>
    <p class="pdesc">Рисунок 20. Ложные связи</p>
</p>

Ложные связи рождают ошибки создания. Помимо этого, ошибка создания может быть просто увеличенной корреляцией. Если вместо $\overset{—}{n}|\overset{—}{k}$ ($\overset{—}{n}$ означает $0\le n \le1$) пара корреляций станет полустабильной $1|\overset{—}{k}$, это будет ошибка включения. Превращение в обратном порядке — ошибка исключения. Определяют тип ошибки вопросы «Какой тип связи?» и «Что произошло?», на которые отвечают первый столбец и строка таблицы соответственно. 

Связь $1|1$, если она ошибочна — ошибка синтеза, аналогично $0|0$ — ошибка деления. Ошибка слияния, созданная из связи $\overset{—}{n}|\overset{—}{k}$, называется *стереотип*. Все итальянцы едят пиццу — это неправда, потому что есть нелюбящие пиццу итальянцы и любящие пиццу неитальянцы — мы исправили две ошибки включения. На самом деле стереотипы являются [стереопаттернами][7] (каламбур).

Ошибки могут рождаться не только из-за создания ложных выводов (например, в результате [безумия][6]), но и при получении опыта. Приведём пример (ниже будет ещё один).

*Ошибка синтеза*: математика — на самом деле математика и логика. Я посвятил разбору этой ошибки [отдельную статью][4]. На первый взгляд кажется, что это ошибка включения, но на самом деле мы считаем, что любая математическая система обладает логикой, а также существует логика без математики. Не понятно, почему тогда всё это вместе ошибочно не называть логикой. Математика же в отдельности — это область, изучающая недискретные величины. И мы думаем иначе, потому что сначала изобрели логику, потом — целые числа, затем их закономерности (арифметику), и лишь потом — иррациональные и комплексные числа.

Все вероятностные ошибки, полученные в результате получения опыта, являются частными случаями ошибки выжившего: для их исправления необходимо получить больше новой информации. Знание *закономерности* — стабильной корреляции — избавляет от этой необходимости. Закономерностями являются и сами законы логики.

## Рекурсии

Чтобы рассмотреть ещё одну ошибку, нужно сперва сделать отступление к яблокам. Утверждение «Яблоко содержит косточки» позволяет разложить множество всех яблок по-разному. Это открывает нам *теорему фрактальности* логики: если несколько подмножеств одного множества содержат одно и то же подмножество, его полезно рассмотреть, как отдельное множество. 

<p>
    <img class="pimg" src="../../../images/PSS/21.jpg" width=400>
    <p class="pdesc">Рисунок 21. В каждом яблоке есть косточка</p>
</p>

Это следствие из *фундаментальной теоремы* логики: вещи можно объединить по общему признаку. Благодаря свойству фрактальности мы можем абстрагироваться от конкретного яблока и изучать косточки всех яблок во Вселенной как один объект. С другой стороны, мы можем объединить яблоки с другими объектами, образовав меты. Метафора — это *перемещение* (см. [операции][15]) связей на другой объект в множестве общего признака — мете. Именно поэтому ни у кого не вызывает удивления, что глазное яблоко называется яблоком, а в английском языке — мячом (eyeball):

<p>
    <img class="pimg" src="../../../images/PSS/22.jpg" width=400>
    <p class="pdesc">Рисунок 22. Глазные арбузы</p>
</p>

*Ошибка деления*: вероятность и корреляция — это одно и то же. Ошибка появилась потому, что мы подбирались к этому открытию с разных сторон, изобрели эти понятия по отдельности, и не успели проложить между ними все связи. Вот почему мы так легко применяем законы вероятности, когда рассматриваем корреляции. Вот знаменитая теорема Байеса:

<p>
    <img class="pimg" src="../../../images/PSS/23.jpg" width=400>
    <p class="pdesc">Рисунок 23. Теорема Байеса</p>
</p>

*Шанс* $c$ — это дробная часть полустабильной корреляции. Мы говорим о шансах, когда все рассматриваемые связи относятся к одной и той же системе, так как $c_i = r_{Si}$. Поэтому шанс относится только к типам. Теорема Байеса содержит в знаменателе ещё одну теорему, которая и позволяет сократить формулу: зная шанс нестабильного множества и корреляции его и его антонима с другим нестабильным множеством, можно посчитать шанс этого множества в системе. 

Полустабильные корреляции от любой системы рождают типы, сумма шансов которых равна $1$, комбинации параметров рождают паттерны, а нестабильные множества, связанные с другими системами, позволяют посчитать эффективность данной системы. Это означает несколько видов рекурсии.

*Стабильная рекурсия* позволяет объединить несколько объектов в один. Если между ним и другими системами совсем нет связей, это значит, что он не зависит от внешних параметров. Примером может служить [реальность][15]. Система, полностью состоящая из стабильных связей, называется формальной (см. [Гусеница формальности][2]):

<p>
    <img class="pimg" src="../../../images/PSS/24.jpg" width=400>
    <p class="pdesc">Рисунок 24. Стабильная рекурсия</p>
</p>

*Полустабильная рекурсия* является классической рекурсией множеств — рекурсией типов:

<p>
    <img class="pimg" src="../../../images/PSS/25.jpg" width=400>
    <p class="pdesc">Рисунок 25. Полустабильная рекурсия</p>
</p>

А примеры *нестабильной рекурсии* мы видели выше, когда обсуждали косточки в яблоке и объекты с одной связью, которую можно провести между учениками, школами, городами и странами:

<p>
    <img class="pimg" src="../../../images/PSS/26.jpg" width=400>
    <p class="pdesc">Рисунок 26. Нестабильная рекурсия</p>
</p>

Объединение этих видов рекурсий создаёт [лингвистическую модель][7], из которой следуют части речи (зелёный и фиолетовый цвета связей означают их создание и разрушение соответственно):

<p>
    <img class="pimg" src="../../../images/PSS/27.jpg" width=400>
    <p class="pdesc">Рисунок 27. Части речи</p>
</p>

Таким образом, Вероятностно-Стратегические Схемы являются объединением стабильного и нестабильного, рационального и иррационального, позволяющим нам мыслить, а также отображающим все законы информационных систем, многие из которых я раскрою в следующих статьях.

## Заключение и прочее

ВСС обладают многими свойствами перцептронных нейронных сетей, при этом имея свои отличия:

1) Несмотря на хаотичность, направление времени явно задано и может быть обращено;
2) Любая группа нейронов может быть осмыслена как множество (тогда *нейрон* — атом информационной системы);
3) Любые множества могут быть связаны, и именно наличие или память об отсутствии этих связей создают опыт;
4) Кажется, что создать статичные сети, решающие задачи вроде распознавания текста с фото, достаточно просто: задать систему отличий разных символов и правила чтения, а также создать [*алгоритм фокуса*][15]: найти то, что напоминает текст, разделить его на абзацы, найти первую строку и первую букву;
5) Для более сложных задач, таких, как оценка вероятности сценариев будущего, осознанная речь и решение бытовых проблем, нужны [*алгоритмы мышления*][15], что требует гораздо больших мощностей.

Я думаю, что, помимо объединения нескольких разделов математики и логики, ВСС объединяет графы с разной толщиной связи и способ мышления, которым пользуются детективы для расследования преступлений в крутых фильмах (и не только), развешивая фотографии и заметки, соединённые красными ниточками.

Есть другой способ графического обозначения корреляций, при котором каждый тип связи однозначно задан и нет необходимости в таком количестве стрелок:

<p>
    <img class="pimg" src="../../../images/PSS/28.jpg" width=800>
    <p class="pdesc">Рисунок 28. Два способа обозначения</p>
</p>

Также стоит поговорить про распространённый тезис «Корреляция не есть причинно-следственная связь». Я даже видел «Корреляция означает совпадение». Вы все видели этот [сайт][Fool trends]. Это не корреляции. Это графики совпадения трендов. Целые цепочки совпадений углов наклона отрезка между двумя точками. Ну да, по закону больших чисел такое бывает, и утверждение о том, что два объекта связаны из-за одинакового числа, звучит нумерологично. Более того, оси имеют разные единицы измерения, а графики растягиваются и сжимаются как угодно, что позволяет создавать совпадения. Настоящие корреляции имеют смысл. Применимо к той же теореме Байеса, если мы знаем, что яблоко входит в $A$, и корреляцию $r_{AB}$, это значит вероятное появление или не появление некоторого яблока Шрёдингера в множестве $B$. Миром правит вероятность:

<p>
    <img class="pimg" src="../../../images/PSS/29.jpg" width=800>
    <p class="pdesc">Рисунок 29. Яблоко Шрёдингера</p>
</p>

Причинно-следственных связей не существует. Я хочу сказать, некорректно думать о том, что одно событие является причиной, а другое — следствием, потому что навык проигрывания времени в обратном направлении крайне полезен. Приведём довольно сложный пример: деревья качаются, потому что ветер дует. Кажется, будто обратное заключение — ветер дует, потому что деревья качаются — абсурд, но, во-первых, это заключение возможно в случае стереотипа — именно так они и работают, а значит, может быть получено от недостатка опыта, а во вторых, исправление ошибки выжившего говорит, что заключение звучит по-другому: если деревья качаются, то с высокой вероятностью причина этому — ветер. Наши знания о ветре также говорят, что деревья всегда качаются, когда ветер дует, но это не значит стереотип. Почему? Мы путаем ветер, как существительное, и его дуновение, как глагол, и между двумя действиями здесь, в отличие от объектов, есть стабильная связь, что создаёт функцию (см. [время][7]):

<p>
    <img class="pimg" src="../../../images/PSS/30.jpg" width=800>
    <p class="pdesc">Рисунок 30. Связь действий</p>
</p>

И уже внутри этой функции можно задать направление мысли между действиями, образовав [функциональную зависимость][15].

## Домашнее задание

В этой части я хочу задать специалистам и всем интересующимся вопросы, на которые важно получить ответы.

1) Ясно, что для задач, которые стоят перед ВСС, можно использовать аналоговые компьютеры. Цифровые компьютеры отняли у них внимание из-за большей применимости для задач, стоявших в середине ХХ века, но сейчас они довольно многообещающи и многозадачны. Каким образом задаются описанные в статье операции на аналоговом компьютере? Можно ли оперировать множествами, то есть создать машину Тьюринга, используя лишь аналоговый компьютер? Как можно уменьшить эффект накопления ошибки, возникающий при аналоговом вычислении?
2) Может ли аналоговый компьютер сопряжён с цифровым? Каким образом будет организована эта система? Как будет обеспечена передача команд с одного компьютера на другой? Кстати, могут ли видеокарты ускорить аналоговые вычисления, и если да, то насколько?
3) Какие минимальные мощности нужны (предварительно) для создания на цифровом компьютере систем, описанных в пунктах 4 и 5 списка отличий от нейросетей? 
4) Как здесь можно применить различные алгоритмы оптимизации графов? Какие ещё законы статистики применимы к ВСС?

